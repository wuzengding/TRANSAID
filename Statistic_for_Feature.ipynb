{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4e63ab-4eb5-4246-971e-5473880df1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e19517-104a-495b-8227-1053f224d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationSiteAnalyzer:\n",
    "    def __init__(self, matching_pkl: str, fasta_file: str, output_dir: str = '.',):\n",
    "        \"\"\"\n",
    "        初始化分析器\n",
    "        \n",
    "        Args:\n",
    "            matching_pkl: matching_predictions.pkl文件路径\n",
    "            fasta_file: GRCh38_latest_rna.fna文件路径\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        # 加载数据\n",
    "        with open(matching_pkl, 'rb') as f:\n",
    "            self.matching_data = pickle.load(f)\n",
    "            \n",
    "        # 加载序列数据\n",
    "        self.sequences = {}\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            self.sequences[record.id.split('.')[0]] = str(record.seq)\n",
    "            \n",
    "        # 设置绘图风格\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        \n",
    "    def analyze_tis_tts_probabilities(self, window_size: int = 6):\n",
    "        \"\"\"分析TIS/TTS位点概率分布和上下文概率\n",
    "    \n",
    "        Args:\n",
    "            window_size: 上下文窗口大小,默认为6\n",
    "        \"\"\"\n",
    "        # 初始化数据存储结构 - 分别存储TIS和TTS位点的位置特异性概率\n",
    "        tis_positions = {f'pos{i}':[] for i in range(-window_size, window_size+4)}\n",
    "        tts_positions = {f'pos{i}':[] for i in range(-window_size, window_size+4)}\n",
    "        \n",
    "        # 新增：存储TIS/TTS三个位点的概率值\n",
    "        tis_site_probs = []  # 每个元素包含连续3个位点的概率值\n",
    "        tts_site_probs = []\n",
    "        \n",
    "        # 新增：存储TIS/TTS上下游概率值\n",
    "        tis_upstream_probs = []  # 存储上游window_size个位点的概率值\n",
    "        tis_downstream_probs = []  # 存储下游window_size个位点的概率值\n",
    "        tts_upstream_probs = []\n",
    "        tts_downstream_probs = []\n",
    "        \n",
    "        # 遍历每个转录本的预测结果\n",
    "        for item in self.matching_data:\n",
    "            probs = item['predictions_probs']  # shape: (seq_len, 3)\n",
    "            true_labels = item['true_labels']  # shape: (seq_len,)\n",
    "            seq_len = item['length']\n",
    "        \n",
    "            # 在序列中寻找连续的TIS和TTS位置\n",
    "            for i in range(window_size, seq_len - window_size - 4):\n",
    "                # 检查是否有连续的3个TIS (label=0)\n",
    "                if (true_labels[i] == 0 and \n",
    "                    true_labels[i+1] == 0 and \n",
    "                    true_labels[i+2] == 0):\n",
    "                    # 收集TIS三个位点的概率值\n",
    "                    site_probs = [probs[i+j] for j in range(3)]\n",
    "                    tis_site_probs.append(site_probs)\n",
    "                    \n",
    "                    # 收集上下游概率值\n",
    "                    upstream_probs = [probs[i+j][0] for j in range(-window_size, 0)]\n",
    "                    downstream_probs = [probs[i+j][0] for j in range(3, window_size+4)]\n",
    "                    tis_upstream_probs.append(upstream_probs)\n",
    "                    tis_downstream_probs.append(downstream_probs)\n",
    "                    \n",
    "                    # 收集所有位置的概率\n",
    "                    for j in range(-window_size, window_size+4):\n",
    "                        if j < 0:  # 前文\n",
    "                            tis_positions[f'pos{j}'].append(probs[i+j][0])\n",
    "                        elif j < 3:  # TIS三个位点\n",
    "                            tis_positions[f'pos{j}'].append(probs[i+j][0])\n",
    "                        else:  # 后文\n",
    "                            tis_positions[f'pos{j}'].append(probs[i+j][0])\n",
    "            \n",
    "                # 检查是否有连续的3个TTS (label=1)\n",
    "                if (true_labels[i] == 1 and \n",
    "                    true_labels[i+1] == 1 and \n",
    "                    true_labels[i+2] == 1):\n",
    "                    # 收集TTS三个位点的概率值\n",
    "                    site_probs = [probs[i+j] for j in range(3)]\n",
    "                    tts_site_probs.append(site_probs)\n",
    "                    \n",
    "                    # 收集上下游概率值\n",
    "                    upstream_probs = [probs[i+j][1] for j in range(-window_size, 0)]\n",
    "                    downstream_probs = [probs[i+j][1] for j in range(3, window_size+4)]\n",
    "                    tts_upstream_probs.append(upstream_probs)\n",
    "                    tts_downstream_probs.append(downstream_probs)\n",
    "                    \n",
    "                    # 收集所有位置的概率\n",
    "                    for j in range(-window_size, window_size+4):\n",
    "                        if j < 0:  # 前文\n",
    "                            tts_positions[f'pos{j}'].append(probs[i+j][1])\n",
    "                        elif j < 3:  # TTS三个位点\n",
    "                            tts_positions[f'pos{j}'].append(probs[i+j][1])\n",
    "                        else:  # 后文\n",
    "                            tts_positions[f'pos{j}'].append(probs[i+j][1])\n",
    "    \n",
    "        # 打印统计信息\n",
    "        print(f\"Found {len(tis_site_probs)} TIS sites\")\n",
    "        print(f\"Found {len(tts_site_probs)} TTS sites\")\n",
    "        \n",
    "        # 绘制图表并保存统计信息\n",
    "        self._plot_site_probabilities(tis_positions, \"TIS\")\n",
    "        self._plot_site_probabilities(tts_positions, \"TTS\")\n",
    "        \n",
    "        # 新增：绘制并保存TIS/TTS三位点概率统计\n",
    "        self._plot_and_save_site_stats(tis_site_probs, \"TIS\")\n",
    "        self._plot_and_save_site_stats(tts_site_probs, \"TTS\")\n",
    "        \n",
    "        # 新增：绘制并保存上下游概率统计\n",
    "        self._plot_and_save_context_stats(tis_upstream_probs, tis_downstream_probs, \"TIS\")\n",
    "        self._plot_and_save_context_stats(tts_upstream_probs, tts_downstream_probs, \"TTS\")\n",
    "        \n",
    "        # 保存概率统计信息\n",
    "        self._print_probability_statistics(tis_positions, \"TIS\")\n",
    "        self._print_probability_statistics(tts_positions, \"TTS\")\n",
    "\n",
    "    def _plot_and_save_site_stats(self, site_probs: List[List], site_type: str):\n",
    "        \"\"\"绘制并保存位点概率统计信息\n",
    "        \n",
    "        Args:\n",
    "            site_probs: 位点概率值列表\n",
    "            site_type: 位点类型 ('TIS' 或 'TTS')\n",
    "        \"\"\"\n",
    "        # 计算均值和方差\n",
    "        site_means = np.mean(site_probs, axis=1)\n",
    "        site_vars = np.var(site_probs, axis=1)\n",
    "        \n",
    "        # 绘制均值分布图\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(site_means, bins=50, alpha=0.7)\n",
    "        plt.title(f'{site_type} Site Probability Mean Distribution')\n",
    "        plt.xlabel('Mean Probability')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{site_type}_site_mean_dist.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 绘制方差分布图\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(site_vars, bins=50, alpha=0.7)\n",
    "        plt.title(f'{site_type} Site Probability Variance Distribution')\n",
    "        plt.xlabel('Variance')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{site_type}_site_var_dist.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 保存统计信息\n",
    "        stats_file = os.path.join(self.output_dir, f'{site_type}_site_stats.txt')\n",
    "        with open(stats_file, 'w') as f:\n",
    "            f.write(f\"{site_type} Site Statistics:\\n\")\n",
    "            f.write(\"=\"*30 + \"\\n\\n\")\n",
    "            f.write(\"Mean Statistics:\\n\")\n",
    "            f.write(f\"Overall Mean: {np.mean(site_means):.4f}\\n\")\n",
    "            f.write(f\"Mean Std: {np.std(site_means):.4f}\\n\")\n",
    "            f.write(f\"Mean Q1: {np.percentile(site_means, 25):.4f}\\n\")\n",
    "            f.write(f\"Mean Q3: {np.percentile(site_means, 75):.4f}\\n\\n\")\n",
    "            f.write(\"Variance Statistics:\\n\")\n",
    "            f.write(f\"Overall Variance: {np.mean(site_vars):.4f}\\n\")\n",
    "            f.write(f\"Variance Std: {np.std(site_vars):.4f}\\n\")\n",
    "            f.write(f\"Variance Q1: {np.percentile(site_vars, 25):.4f}\\n\")\n",
    "            f.write(f\"Variance Q3: {np.percentile(site_vars, 75):.4f}\\n\")\n",
    "    \n",
    "    def _plot_and_save_context_stats(self, upstream_probs: List[List], \n",
    "                                    downstream_probs: List[List],\n",
    "                                    site_type: str):\n",
    "        \"\"\"绘制并保存上下游概率统计信息\n",
    "        \n",
    "        Args:\n",
    "            upstream_probs: 上游概率值列表\n",
    "            downstream_probs: 下游概率值列表\n",
    "            site_type: 位点类型 ('TIS' 或 'TTS')\n",
    "        \"\"\"\n",
    "        # 计算上下游均值和方差\n",
    "        up_means = np.mean(upstream_probs, axis=1)\n",
    "        up_vars = np.var(upstream_probs, axis=1)\n",
    "        down_means = np.mean(downstream_probs, axis=1)\n",
    "        down_vars = np.var(downstream_probs, axis=1)\n",
    "        \n",
    "        # 绘制上游概率分布图\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(up_means, bins=50, alpha=0.7, label='Upstream')\n",
    "        plt.hist(down_means, bins=50, alpha=0.7, label='Downstream')\n",
    "        plt.title(f'{site_type} Context Mean Distribution')\n",
    "        plt.xlabel('Mean Probability')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(up_vars, bins=50, alpha=0.7, label='Upstream')\n",
    "        plt.hist(down_vars, bins=50, alpha=0.7, label='Downstream')\n",
    "        plt.title(f'{site_type} Context Variance Distribution')\n",
    "        plt.xlabel('Variance')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{site_type}_context_dist.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 保存统计信息\n",
    "        stats_file = os.path.join(self.output_dir, f'{site_type}_context_stats.txt')\n",
    "        with open(stats_file, 'w') as f:\n",
    "            f.write(f\"{site_type} Context Statistics:\\n\")\n",
    "            f.write(\"=\"*30 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"Upstream Statistics:\\n\")\n",
    "            f.write(\"-\"*20 + \"\\n\")\n",
    "            f.write(f\"Mean: {np.mean(up_means):.4f}\\n\")\n",
    "            f.write(f\"Std: {np.std(up_means):.4f}\\n\")\n",
    "            f.write(f\"Variance Mean: {np.mean(up_vars):.4f}\\n\")\n",
    "            f.write(f\"Variance Std: {np.std(up_vars):.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Downstream Statistics:\\n\")\n",
    "            f.write(\"-\"*20 + \"\\n\")\n",
    "            f.write(f\"Mean: {np.mean(down_means):.4f}\\n\")\n",
    "            f.write(f\"Std: {np.std(down_means):.4f}\\n\")\n",
    "            f.write(f\"Variance Mean: {np.mean(down_vars):.4f}\\n\")\n",
    "            f.write(f\"Variance Std: {np.std(down_vars):.4f}\\n\")\n",
    "    \n",
    "    def _plot_site_probabilities(self, position_data, site_type):\n",
    "        \"\"\"绘制位点概率分布图\n",
    "       \n",
    "        Args:\n",
    "            position_data: 位置特异性概率数据\n",
    "            site_type: 位点类型 ('TIS' 或 'TTS')\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 4))\n",
    "       \n",
    "        # 准备数据\n",
    "        positions = sorted(position_data.keys(), key=lambda x: int(x[3:]))  # 按位置排序\n",
    "        data = [position_data[pos] for pos in positions]\n",
    "       \n",
    "        # 绘制violin plot\n",
    "        vp = plt.violinplot(data, positions=[i for i in range(len(positions))], \n",
    "                          showmeans=True, showextrema=True)\n",
    "       \n",
    "        # 设置violin plot颜色\n",
    "        for pc in vp['bodies']:\n",
    "            pc.set_facecolor('lightblue')\n",
    "            pc.set_alpha(0.7)\n",
    "       \n",
    "        # 添加scatter plot\n",
    "        for i, d in enumerate(data):\n",
    "            scatter_x = np.random.normal(i, 0.05, size=len(d))\n",
    "            plt.scatter(scatter_x, d, alpha=0.2, c='navy', s=5)\n",
    "       \n",
    "        # 设置图表属性\n",
    "        plt.title(f'{site_type} Position-specific Probability Distribution')\n",
    "        plt.xlabel('Position relative to site')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xticks(range(len(positions)), [p[3:] for p in positions], rotation=45)\n",
    "        plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # 保存图表\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{site_type}_position_distribution.png'), dpi=300)\n",
    "        plt.close()\n",
    "   \n",
    "    def _print_probability_statistics(self, position_data, site_type):\n",
    "        \"\"\"输出概率统计信息到文件\n",
    "        \n",
    "        Args:\n",
    "            position_data: 位置特异性概率数据\n",
    "            site_type: 位点类型 ('TIS' 或 'TTS')\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.output_dir, f'{site_type}_statistics.txt'), 'w') as f:\n",
    "            f.write(f\"{site_type} Probability Statistics:\\n\")\n",
    "            f.write(\"=\"*30 + \"\\n\\n\")\n",
    "           \n",
    "            for pos in sorted(position_data.keys(), key=lambda x: int(x[3:])):\n",
    "                values = position_data[pos]\n",
    "                if values:\n",
    "                    f.write(f\"\\nPosition {pos}:\\n\")\n",
    "                    f.write(f\"Count: {len(values)}\\n\")\n",
    "                    f.write(f\"Mean: {np.mean(values):.4f}\\n\")\n",
    "                    f.write(f\"Std: {np.std(values):.4f}\\n\")\n",
    "                    f.write(f\"Median: {np.median(values):.4f}\\n\")\n",
    "                    f.write(f\"Q1: {np.percentile(values, 25):.4f}\\n\")\n",
    "                    f.write(f\"Q3: {np.percentile(values, 75):.4f}\\n\")\n",
    "                    f.write(\"-\"*20 + \"\\n\")\n",
    "\n",
    "    def analyze_kozak_sequences(self, file_kozak: str):\n",
    "        \"\"\"分析Kozak序列得分\"\"\"\n",
    "        # 读取分组信息\n",
    "        group_a = []\n",
    "        group_b = []\n",
    "        with open(file_kozak, 'r') as f:\n",
    "            header = f.readline().strip().split(';')\n",
    "            for line in f:\n",
    "                a, b = line.strip().split(';')\n",
    "                if a:\n",
    "                    group_a.append(tuple(a.split(',')))\n",
    "                if b:\n",
    "                    group_b.append(tuple(b.split(',')))\n",
    "        \n",
    "        # 计算Kozak得分\n",
    "        scores_a = self._calculate_kozak_scores(group_a)\n",
    "        scores_b = self._calculate_kozak_scores(group_b)\n",
    "        \n",
    "        # 绘制对比图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 绘制小提琴图\n",
    "        violin_parts = plt.violinplot([scores_a, scores_b], \n",
    "                                    positions=[1, 2])\n",
    "        \n",
    "        # 添加散点\n",
    "        plt.scatter(np.ones_like(scores_a), scores_a, \n",
    "                   alpha=0.2, color='blue')\n",
    "        plt.scatter(np.ones_like(scores_b) * 2, scores_b, \n",
    "                   alpha=0.2, color='red')\n",
    "        \n",
    "        plt.xticks([1, 2], header)\n",
    "        plt.ylabel('Kozak Score')\n",
    "        plt.title('Kozak Sequence Score Distribution')\n",
    "        plt.savefig(os.path.join(self.output_dir,'kozak_scores.png'))\n",
    "        plt.close()\n",
    "        \n",
    "    def _calculate_kozak_scores(self, group_data: List[Tuple[str, str]]) -> List[float]:\n",
    "        \"\"\"计算Kozak序列得分\"\"\"\n",
    "        pwm = self._build_pwm()\n",
    "        scores = []\n",
    "        \n",
    "        for transcript_id, pos in group_data:\n",
    "            if transcript_id in self.sequences:\n",
    "                pos = int(pos)\n",
    "                if pos >= 6 and pos + 4 < len(self.sequences[transcript_id]):\n",
    "                    seq = self.sequences[transcript_id][pos-6:pos+4]\n",
    "                    score = self._score_sequence(seq, pwm)\n",
    "                    scores.append(score)\n",
    "                    \n",
    "        return scores\n",
    "\n",
    "    def analyze_cai_and_length(self, file_cai: str):\n",
    "        \"\"\"分析CAI值和CDS长度分布\"\"\"\n",
    "        # 读取分组信息\n",
    "        group_a = []\n",
    "        group_b = []\n",
    "        with open(file_cai, 'r') as f:\n",
    "            header = f.readline().strip().split(';')\n",
    "            for line in f:\n",
    "                a, b = line.strip().split(';')\n",
    "                if a:\n",
    "                    group_a.append(tuple(a.split(',')))\n",
    "                if b:\n",
    "                    group_b.append(tuple(b.split(',')))\n",
    "        \n",
    "        # 计算CAI值和长度\n",
    "        cai_a, len_a = self._calculate_cai_and_length(group_a)\n",
    "        cai_b, len_b = self._calculate_cai_and_length(group_b)\n",
    "        \n",
    "        # 绘制CAI分布对比图\n",
    "        self._plot_distribution_comparison(\n",
    "            cai_a, cai_b, header,\n",
    "            'CAI Value Distribution',\n",
    "            'CAI Value',\n",
    "            'cai_distribution.png'\n",
    "        )\n",
    "        \n",
    "        # 绘制长度分布对比图\n",
    "        self._plot_distribution_comparison(\n",
    "            len_a, len_b, header,\n",
    "            'CDS Length Distribution',\n",
    "            'Length (bp)',\n",
    "            'cds_length_distribution.png'\n",
    "        )\n",
    "        \n",
    "    def _calculate_cai_and_length(self, group_data: List[Tuple[str, str]]):\n",
    "        codon_usage = self._get_codon_usage()\n",
    "        cai_values = []\n",
    "        lengths = []\n",
    "        \n",
    "        for transcript_id, pos_range in group_data:\n",
    "            if transcript_id in self.sequences:\n",
    "                start, end = map(int, pos_range.split('-'))\n",
    "                cds = self.sequences[transcript_id][start:end]\n",
    "                \n",
    "                # 计算CAI\n",
    "                cai = self._calculate_cai(cds, codon_usage)\n",
    "                cai_values.append(cai)\n",
    "                \n",
    "                # 记录长度\n",
    "                lengths.append(end - start)\n",
    "                \n",
    "        return cai_values, lengths\n",
    "        \n",
    "    def _plot_distribution_comparison(self, data_a: List, data_b: List, \n",
    "                                    labels: List[str], title: str, \n",
    "                                    ylabel: str, filename: str):\n",
    "        \"\"\"绘制分布对比图\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 绘制小提琴图\n",
    "        violin_parts = plt.violinplot([data_a, data_b], \n",
    "                                    positions=[1, 2])\n",
    "        \n",
    "        # 添加散点\n",
    "        plt.scatter(np.ones_like(data_a), data_a, \n",
    "                   alpha=0.2, color='blue')\n",
    "        plt.scatter(np.ones_like(data_b) * 2, data_b, \n",
    "                   alpha=0.2, color='red')\n",
    "        \n",
    "        plt.xticks([1, 2], labels)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    def _build_pwm(self) -> Dict:\n",
    "        \"\"\"构建PWM(Position Weight Matrix)矩阵\"\"\"\n",
    "        # 标准Kozak序列PWM\n",
    "        return {\n",
    "            -6: {'A': 0.22, 'C': 0.28, 'G': 0.32, 'T': 0.18},\n",
    "            -5: {'A': 0.20, 'C': 0.30, 'G': 0.30, 'T': 0.20},\n",
    "            -4: {'A': 0.18, 'C': 0.32, 'G': 0.30, 'T': 0.20},\n",
    "            -3: {'A': 0.25, 'C': 0.15, 'G': 0.45, 'T': 0.15}, # 重要位点\n",
    "            -2: {'A': 0.20, 'C': 0.35, 'G': 0.25, 'T': 0.20},\n",
    "            -1: {'A': 0.20, 'C': 0.35, 'G': 0.25, 'T': 0.20},\n",
    "            1:  {'A': 0.20, 'C': 0.20, 'G': 0.40, 'T': 0.20}, # ATG后第一个位点\n",
    "            2:  {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25},\n",
    "            3:  {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
    "        }\n",
    "\n",
    "    def _score_sequence(self, seq: str, pwm: Dict) -> float:\n",
    "        \"\"\"计算序列的PWM得分\"\"\"\n",
    "        score = 1.0\n",
    "        for i, base in enumerate(seq):\n",
    "            pos = i - 6  # 相对于ATG的位置\n",
    "            if pos in pwm and base in pwm[pos]:\n",
    "                score *= pwm[pos][base]\n",
    "        return score\n",
    "\n",
    "    def _get_codon_usage(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            # 标准人类密码子使用频率\n",
    "            'TTT': 0.45, 'TTC': 0.55, 'TTA': 0.07, 'TTG': 0.13,\n",
    "            'TCT': 0.18, 'TCC': 0.22, 'TCA': 0.15, 'TCG': 0.06,\n",
    "            'TAT': 0.43, 'TAC': 0.57, 'TAA': 0.28, 'TAG': 0.20,\n",
    "            'TGT': 0.45, 'TGC': 0.55, 'TGA': 0.52, 'TGG': 1.00,\n",
    "            'CTT': 0.13, 'CTC': 0.20, 'CTA': 0.07, 'CTG': 0.41,\n",
    "            'CCT': 0.28, 'CCC': 0.33, 'CCA': 0.27, 'CCG': 0.11,\n",
    "            'CAT': 0.41, 'CAC': 0.59, 'CAA': 0.25, 'CAG': 0.75,\n",
    "            'CGT': 0.08, 'CGC': 0.19, 'CGA': 0.11, 'CGG': 0.21,\n",
    "            'ATT': 0.36, 'ATC': 0.48, 'ATA': 0.16, 'ATG': 1.00,\n",
    "            'ACT': 0.24, 'ACC': 0.36, 'ACA': 0.28, 'ACG': 0.12,\n",
    "            'AAT': 0.46, 'AAC': 0.54, 'AAA': 0.42, 'AAG': 0.58,\n",
    "            'AGT': 0.15, 'AGC': 0.24, 'AGA': 0.20, 'AGG': 0.20,\n",
    "            'GTT': 0.18, 'GTC': 0.24, 'GTA': 0.11, 'GTG': 0.47,\n",
    "            'GCT': 0.26, 'GCC': 0.40, 'GCA': 0.23, 'GCG': 0.11,\n",
    "            'GAT': 0.46, 'GAC': 0.54, 'GAA': 0.42, 'GAG': 0.58,\n",
    "            'GGT': 0.16, 'GGC': 0.34, 'GGA': 0.25, 'GGG': 0.25\n",
    "        }\n",
    "\n",
    "    def _calculate_cai(self, sequence: str, codon_usage: Dict[str, float]) -> float:\n",
    "        \"\"\"计算序列的CAI值\"\"\"\n",
    "        if len(sequence) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        # 将序列按三联体分割\n",
    "        codons = [sequence[i:i+3] for i in range(0, len(sequence)-2, 3)]\n",
    "    \n",
    "        # 计算几何平均值\n",
    "        values = []\n",
    "        for codon in codons:\n",
    "            if codon in codon_usage:\n",
    "                values.append(codon_usage[codon])\n",
    "    \n",
    "        if not values:\n",
    "            return 0.0\n",
    "        \n",
    "        # 计算几何平均值\n",
    "        return np.exp(np.mean(np.log(values)))\n",
    "\n",
    "    def analyze_sequence_features(self):\n",
    "        \"\"\"分析序列特征并生成报告\"\"\"\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "        # 生成统计报告\n",
    "        report_path = os.path.join(self.output_dir, 'sequence_analysis_report.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"Sequence Analysis Report\\n\")\n",
    "            f.write(\"=======================\\n\\n\")\n",
    "        \n",
    "            # 1. 基本统计信息\n",
    "            f.write(\"1. Basic Statistics\\n\")\n",
    "            f.write(\"-----------------\\n\")\n",
    "            f.write(f\"Total sequences analyzed: {len(self.matching_data)}\\n\")\n",
    "        \n",
    "            # 按转录本类型统计\n",
    "            transcript_types = defaultdict(int)\n",
    "            for item in self.matching_data:\n",
    "                trans_type = item['transcript_id'].split('_')[0]\n",
    "                transcript_types[trans_type] += 1\n",
    "            \n",
    "            f.write(\"\\nTranscript Type Distribution:\\n\")\n",
    "            for t_type, count in transcript_types.items():\n",
    "                f.write(f\"{t_type}: {count} ({count/len(self.matching_data)*100:.2f}%)\\n\")\n",
    "            \n",
    "            # 2. 序列长度分布\n",
    "            f.write(\"\\n2. Sequence Length Distribution\\n\")\n",
    "            f.write(\"----------------------------\\n\")\n",
    "            lengths = [item['length'] for item in self.matching_data]\n",
    "            f.write(f\"Mean length: {np.mean(lengths):.2f}\\n\")\n",
    "            f.write(f\"Median length: {np.median(lengths):.2f}\\n\")\n",
    "            f.write(f\"Std length: {np.std(lengths):.2f}\\n\")\n",
    "            f.write(f\"Min length: {min(lengths)}\\n\")\n",
    "            f.write(f\"Max length: {max(lengths)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae0c8b6-279b-47fe-9c54-732ad98fabe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12009 TIS sites\n",
      "Found 12029 TTS sites\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/Statictis\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化分析器\n",
    "    analyzer = TranslationSiteAnalyzer(\n",
    "        matching_pkl=\"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/TRANSAID_Embedding_batch4_NM_matching_predictions.pkl\",\n",
    "        fasta_file=\"/home/jovyan/work/insilico_translation/dataset/GRCh38_latest_rna.fna\",\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # 分析TIS/TTS概率分布\n",
    "    analyzer.analyze_tis_tts_probabilities()\n",
    "    \n",
    "    # 分析Kozak序列\n",
    "    analyzer.analyze_kozak_sequences(\"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/Statictis/kozak_config_file.txt\")\n",
    "    \n",
    "    # 分析CAI和CDS长度\n",
    "    analyzer.analyze_cai_and_length(\"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/Statictis/CAI_CDS_config_file.txt\")\n",
    "    \n",
    "    # 生成综合报告\n",
    "    analyzer.analyze_sequence_features()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5580f04a-08a4-46ff-9bcb-730995f5ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def load_pickle_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"加载pickle文件数据\n",
    "    \n",
    "    Args:\n",
    "        file_path: pickle文件路径\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: 加载的数据列表\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def find_first_tis_position(array: np.ndarray) -> int:\n",
    "    \"\"\"查找第一个连续三个0的起始位置\n",
    "    \n",
    "    Args:\n",
    "        array: 预测或true_labels数组\n",
    "    \n",
    "    Returns:\n",
    "        int: 第一个TIS的起始位置，如果没找到返回-1\n",
    "    \"\"\"\n",
    "    for i in range(len(array)-2):\n",
    "        if array[i] == 0 and array[i+1] == 0 and array[i+2] == 0:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def collect_group_a_data(nm_data: List[Dict]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"收集group A数据（基于true_labels）\n",
    "    \n",
    "    Args:\n",
    "        nm_data: NM转录本预测数据\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: 转录本ID和TIS位置的列表\n",
    "    \"\"\"\n",
    "    group_a = []\n",
    "    for item in nm_data:\n",
    "        true_labels = np.array(item['true_labels'])\n",
    "        transcript_id = item['transcript_id']\n",
    "        \n",
    "        # 查找所有TIS位置\n",
    "        tis_positions = []\n",
    "        for i in range(len(true_labels)-2):\n",
    "            if (true_labels[i:i+3] == 0).all():\n",
    "                tis_positions.append(i)\n",
    "        \n",
    "        if len(tis_positions) > 1:\n",
    "            warnings.warn(f\"Multiple TIS found in transcript {transcript_id}: {tis_positions}\")\n",
    "            # 只取第一个TIS位置\n",
    "            group_a.append((transcript_id, tis_positions[0]))\n",
    "        elif len(tis_positions) == 1:\n",
    "            group_a.append((transcript_id, tis_positions[0]))\n",
    "    \n",
    "    return group_a\n",
    "\n",
    "def collect_group_b_data(nr_data: List[Dict]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"收集group B数据（基于predictions）\n",
    "    \n",
    "    Args:\n",
    "        nr_data: NR转录本预测数据\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: 转录本ID和TIS位置的列表\n",
    "    \"\"\"\n",
    "    group_b = []\n",
    "    for item in nr_data:\n",
    "        predictions = np.array(item['predictions'])\n",
    "        transcript_id = item['transcript_id']\n",
    "        \n",
    "        # 查找第一个连续三个0的位置\n",
    "        tis_pos = find_first_tis_position(predictions)\n",
    "        if tis_pos != -1:\n",
    "            group_b.append((transcript_id, tis_pos))\n",
    "    \n",
    "    return group_b\n",
    "\n",
    "def save_to_file(group_a: List[Tuple[str, int]], \n",
    "                 group_b: List[Tuple[str, int]], \n",
    "                 output_file: str):\n",
    "    \"\"\"保存数据到文件\n",
    "    \n",
    "    Args:\n",
    "        group_a: group A数据\n",
    "        group_b: group B数据\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    # 确定两组数据的最大长度\n",
    "    max_len = max(len(group_a), len(group_b))\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        # 写入表头\n",
    "        f.write('group_A;group_B\\n')\n",
    "        \n",
    "        # 写入数据\n",
    "        for i in range(max_len):\n",
    "            line_parts = []\n",
    "            \n",
    "            # Group A数据\n",
    "            if i < len(group_a):\n",
    "                line_parts.append(f\"{group_a[i][0]},{group_a[i][1]}\")\n",
    "            else:\n",
    "                line_parts.append(\"\")\n",
    "                \n",
    "            # Group B数据\n",
    "            if i < len(group_b):\n",
    "                line_parts.append(f\"{group_b[i][0]},{group_b[i][1]}\")\n",
    "            else:\n",
    "                line_parts.append(\"\")\n",
    "            \n",
    "            f.write(f\"{line_parts[0]};{line_parts[1]}\\n\")\n",
    "\n",
    "def main():\n",
    "    # 设置文件路径\n",
    "    nm_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/TRANSAID_Embedding_batch4_NM_matching_predictions.pkl\"\n",
    "    nr_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/TRANSAID_Embedding_batch4_NR_non_matching_predictions.pkl\"\n",
    "    output_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/Statictis/kozak_config_file.txt\"\n",
    "    \n",
    "    # 加载数据\n",
    "    print(\"Loading NM data...\")\n",
    "    nm_data = load_pickle_data(nm_file)\n",
    "    print(\"Loading NR data...\")\n",
    "    nr_data = load_pickle_data(nr_file)\n",
    "    \n",
    "    # 收集数据\n",
    "    print(\"Collecting group A data...\")\n",
    "    group_a = collect_group_a_data(nm_data)\n",
    "    print(f\"Found {len(group_a)} TIS sites in group A\")\n",
    "    \n",
    "    print(\"Collecting group B data...\")\n",
    "    group_b = collect_group_b_data(nr_data)\n",
    "    print(f\"Found {len(group_b)} TIS sites in group B\")\n",
    "    \n",
    "    # 保存结果\n",
    "    print(\"Saving results...\")\n",
    "    save_to_file(group_a, group_b, output_file)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    print(\"skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1c32c4-b72e-43ef-ab0f-1899bd1d288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def load_pickle_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"加载pickle文件数据\n",
    "    \n",
    "    Args:\n",
    "        file_path: pickle文件路径\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: 加载的数据列表\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def find_first_tis_position(array: np.ndarray) -> int:\n",
    "    \"\"\"查找第一个连续三个0的起始位置\n",
    "    \n",
    "    Args:\n",
    "        array: 预测或true_labels数组\n",
    "    \n",
    "    Returns:\n",
    "        int: 第一个TIS的起始位置，如果没找到返回-1\n",
    "    \"\"\"\n",
    "    for i in range(len(array)-2):\n",
    "        if array[i] == 0 and array[i+1] == 0 and array[i+2] == 0:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_first_tts_end_position(array: np.ndarray) -> int:\n",
    "    \"\"\"查找第一个连续三个1的结束位置\n",
    "    \n",
    "    Args:\n",
    "        array: 预测或true_labels数组\n",
    "    \n",
    "    Returns:\n",
    "        int: 第一个TTS的结束位置，如果没找到返回-1\n",
    "    \"\"\"\n",
    "    for i in range(len(array)-2):\n",
    "        if array[i] == 1 and array[i+1] == 1 and array[i+2] == 1:\n",
    "            return i + 2  # 返回连续三个1的最后一个位置\n",
    "    return -1\n",
    "\n",
    "def collect_group_a_data(nm_data: List[Dict]) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "    \"\"\"收集group A数据（基于true_labels）\n",
    "    \n",
    "    Args:\n",
    "        nm_data: NM转录本预测数据\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, Tuple[int, int]]]: 转录本ID和TIS-TTS位置对的列表\n",
    "    \"\"\"\n",
    "    group_a = []\n",
    "    for item in nm_data:\n",
    "        true_labels = np.array(item['true_labels'])\n",
    "        transcript_id = item['transcript_id']\n",
    "        \n",
    "        # 查找TIS位置\n",
    "        tis_pos = find_first_tis_position(true_labels)\n",
    "        # 查找TTS位置\n",
    "        tts_pos = find_first_tts_end_position(true_labels)\n",
    "        \n",
    "        if tis_pos != -1 and tts_pos != -1:\n",
    "            group_a.append((transcript_id, (tis_pos, tts_pos)))\n",
    "    \n",
    "    return group_a\n",
    "\n",
    "def collect_group_b_data(nr_data: List[Dict]) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "    \"\"\"收集group B数据（基于predictions）\n",
    "    \n",
    "    Args:\n",
    "        nr_data: NR转录本预测数据\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, Tuple[int, int]]]: 转录本ID和TIS-TTS位置对的列表\n",
    "    \"\"\"\n",
    "    group_b = []\n",
    "    for item in nr_data:\n",
    "        predictions = np.array(item['predictions'])\n",
    "        transcript_id = item['transcript_id']\n",
    "        \n",
    "        # 查找TIS位置\n",
    "        tis_pos = find_first_tis_position(predictions)\n",
    "        # 查找TTS位置\n",
    "        tts_pos = find_first_tts_end_position(predictions)\n",
    "        \n",
    "        if tis_pos != -1 and tts_pos != -1:\n",
    "            group_b.append((transcript_id, (tis_pos, tts_pos)))\n",
    "    \n",
    "    return group_b\n",
    "\n",
    "def save_to_file(group_a: List[Tuple[str, Tuple[int, int]]], \n",
    "                 group_b: List[Tuple[str, Tuple[int, int]]], \n",
    "                 output_file: str):\n",
    "    \"\"\"保存数据到文件\n",
    "    \n",
    "    Args:\n",
    "        group_a: group A数据\n",
    "        group_b: group B数据\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    # 确定两组数据的最大长度\n",
    "    max_len = max(len(group_a), len(group_b))\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        # 写入表头\n",
    "        f.write('group_A;group_B\\n')\n",
    "        \n",
    "        # 写入数据\n",
    "        for i in range(max_len):\n",
    "            line_parts = []\n",
    "            \n",
    "            # Group A数据\n",
    "            if i < len(group_a):\n",
    "                transcript_id, (tis_pos, tts_pos) = group_a[i]\n",
    "                line_parts.append(f\"{transcript_id},{tis_pos}-{tts_pos}\")\n",
    "            else:\n",
    "                line_parts.append(\"\")\n",
    "                \n",
    "            # Group B数据\n",
    "            if i < len(group_b):\n",
    "                transcript_id, (tis_pos, tts_pos) = group_b[i]\n",
    "                line_parts.append(f\"{transcript_id},{tis_pos}-{tts_pos}\")\n",
    "            else:\n",
    "                line_parts.append(\"\")\n",
    "            \n",
    "            f.write(f\"{line_parts[0]};{line_parts[1]}\\n\")\n",
    "\n",
    "def main():\n",
    "    # 设置文件路径\n",
    "    nm_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/TRANSAID_Embedding_batch4_NM_matching_predictions.pkl\"\n",
    "    nr_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/TRANSAID_Embedding_batch4_NR_non_matching_predictions.pkl\"\n",
    "    output_file = \"/home/jovyan/work/insilico_translation/embedding_type3_maxlen9995_ratio80_NM_NR/Statictis/CAI_CDS_config_file.txt\"\n",
    "    \n",
    "    # 加载数据\n",
    "    print(\"Loading NM data...\")\n",
    "    nm_data = load_pickle_data(nm_file)\n",
    "    print(\"Loading NR data...\")\n",
    "    nr_data = load_pickle_data(nr_file)\n",
    "    \n",
    "    # 收集数据\n",
    "    print(\"Collecting group A data...\")\n",
    "    group_a = collect_group_a_data(nm_data)\n",
    "    print(f\"Found {len(group_a)} valid TIS-TTS pairs in group A\")\n",
    "    \n",
    "    print(\"Collecting group B data...\")\n",
    "    group_b = collect_group_b_data(nr_data)\n",
    "    print(f\"Found {len(group_b)} valid TIS-TTS pairs in group B\")\n",
    "    \n",
    "    # 保存结果\n",
    "    print(\"Saving results...\")\n",
    "    save_to_file(group_a, group_b, output_file)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    print(\"skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7f6cb-2f18-46d3-8af2-51cc0dc9d7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
